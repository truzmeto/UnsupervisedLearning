---
title: "CS7641 Macine Learning Assignment III by Georgia Tech: Unsupervised Learning"
author: "T. Ruzmetov"
date: "October 29, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction


## K-means Clustering

Randomly initilize K centroids in a given space (2D, 3D, ..), where N-features defines dimention. Then for each data point calculate distance to
all centroids and choose the closest centroid to assign a label. After assignment is complete, move each centroid to the
geometrical center of corresponding class(label). Reitarate until locations of centroids converge to some position.

In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it is prone to fall into local minima.
That's why it can be useful to restart it several times.

The k-means++ algorithm offers a smart procedure to initialize the cluster centers, where it guarantees to find a solution that is O(log k)
competitive to the optimal k-means solution.



## Expectation Maximization Algorithm and Gaussian Mixture Models

Expectation maximization (EM) is a numerical method for maximum likelihood estimation. EM guarantees to approach a local or
global optima by increasing maximum likelihood of the data with subsequent iteration.  
Here we use EM with gaussian mixture model. The algoritm basically consists of two main steps, where in a first step known as Expectation step, 
we calculate expectation of the component assignment(labels) for each data point given the model parameters(gaussian parametrs: means $\mu_{k}$,
weights $\phi_{k}$, standart deviations $\sigma_{k}$). The second step is Maximization step, which consists of maximizing the expectations calculated
in a first step with respect to the model parameters. This step involves updating above mationed parameters iteratively. The entire process repeats
until the algorithm converges to maximum likelihood estimate.


## Dimentionality Reduction: PCA, ICA, RPA, PCA Kernel Trick ...

**PCA** 
Principal component analysis (PCA) is a dimentionality reduction technique which converts a set of possibly correlated variables(features)
into principal components which happens to be directions that capture most variance in data. For instance, if I have a data set composed of
features $X_1=f(x)$, $X_2=g(x)$ and $X_3 = a*X_1 + b*X_2$ that is linear combination of 1st and second features. Then, applying PCA to above
data can transform it from 3D to 2D. It is abvious that 1st and second features are sufficient to explain all variance in the model and
3rd feature is just a linear combination of those two. 

**ICA**
Independent Component Analysis (ICA) is a powerful  technique to separate linearly mixed sources. It transforms original
features, assuming they are linearly mixed, into independent components with zero mutual information. 
The latent variables are assumed nongaussian and mutually independent, and they are called the independent
components of the observed data. After applying ICA to whole data number of components can be reduced by choosing components with highest
kurtosis, which is the fourth central moment divided by the square of the variance.

**Random Projection**

Random projection is a technique used to reduce the dimensionality. Method is powerful, simple and good at preserving distances well after
projection. In random projection, the original d-dimensional data is projected to a k-dimensional (k << d) subspace, using a random k × d 
- dimensional matrix R whose rows have unit lengths.

Using matrix notation: If X d × N is the original set of N d-dimensional observations, then X k × N R P = R k × d X d × N is the projection
of the data onto a lower k-dimensional subspace. Random projection is computationally simple: form the random matrix "R" and project the
d × N data matrix X onto K dimensions of order O ( d k N ) 



## Conclusion






